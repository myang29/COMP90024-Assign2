{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('anger.n.01'), Synset('anger.n.02'), Synset('wrath.n.02'), Synset('anger.v.01'), Synset('anger.v.02')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "\n",
    "syns = wn.synsets(\"anger\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry']\n",
      "[Sentence(\"angry\")]\n",
      "[('angry', 'JJ')]\n",
      "Sentiment(polarity=-0.5, subjectivity=1.0)\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "example = TextBlob(\"angry\")\n",
    "print(example.words)\n",
    "print(example.sentences)\n",
    "print(example.tags)\n",
    "print(example.sentiment)\n",
    "print(example.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Oscars, Lady Gaga trained with a vocal coach DAILY for 6 months  #melbourne \n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n"
     ]
    }
   ],
   "source": [
    "lineNumber = 1\n",
    "with open('tinyTwitter.json', \"r\") as f:\n",
    "    # read metadata from first row\n",
    "    # {\"total_rows\":3877777,\"offset\":805584,\"rows\":[\n",
    "    firstRow = json.loads(f.readline().rstrip()[:-1] + '0}')\n",
    "    numRows = firstRow['total_rows']\n",
    "    \n",
    "    twitLine = f.readline()\n",
    "    while twitLine:  # while not end of file\n",
    "        #lineNumber += 1\n",
    "        # truncate the end of line (\\n)\n",
    "        twitLine = twitLine.rstrip()\n",
    "        if twitLine[-1] == ',':\n",
    "            # truncate the last character\n",
    "            # print('\",\" detected')\n",
    "            twitLine = twitLine[:-1]\n",
    "        if twitLine[0] == ']':\n",
    "            # ignore the last line ']}'\n",
    "            break\n",
    "\n",
    "        # print (\"process {} is processing ...\".format(rank))\n",
    "        twit = json.loads(twitLine)\n",
    "        twit_text = re.sub(r\"http\\S+\",\"\",twit['value']['properties']['text'])\n",
    "        print (twit_text)\n",
    "        blob = TextBlob(twit_text)\n",
    "        print (blob.sentiment)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'geometry': {'coordinates': [[[[143.817818144, -37.56100644], [143.8174912, -37.562841825], [143.817114528, -37.564763975], [143.81697561599998, -37.56545293350001], [143.81694867200002, -37.56565093900001], [143.81670928, -37.566769874500004], [143.81621244800002, -37.56861833900001], [143.81617311999997, -37.568888217], [143.815870752, -37.570438887], [143.810498528, -37.5698114965], [143.81040195200003, -37.5698978175], [143.801001568, -37.5687896675], [143.79427222399994, -37.56800291800001], [143.78298697600002, -37.5666879565], [143.78281488000002, -37.5666710475], [143.75557145599998, -37.56348046349999], [143.748011008, -37.562609002500004], [143.738198016, -37.5614719925], [143.71563801599999, -37.558854002], [143.707238272, -37.5578780345], [143.69367420799995, -37.5582220235], [143.692926464, -37.558273953], [143.69230656000002, -37.558404710999994], [143.68592915200003, -37.56013037250001], [143.68494601600003, -37.551535920000006], [143.68694112, -37.5407070375], [143.68699801600002, -37.54046600099999], [143.68885488, -37.530731079], [143.68925916799998, -37.5307830085], [143.690048288, -37.526354016000006], [143.690823008, -37.521999356500004], [143.69083488, -37.521911074500004], [143.69194511999999, -37.52203698550001], [143.69195951999998, -37.52194742699999], [143.692952032, -37.5163499745], [143.69296681600002, -37.5162609525], [143.70365100799995, -37.5175800025], [143.70397299199996, -37.517608992], [143.70445001599998, -37.518707004], [143.70466601599998, -37.519108010000004], [143.70476799999997, -37.5193530055], [143.70482099199998, -37.518987001499994], [143.70557507199996, -37.515013664], [143.706168, -37.511888995499994], [143.70626, -37.5114419985], [143.70935926399994, -37.51284872], [143.70985798399997, -37.5130749935], [143.71504499199997, -37.5146030009999], [143.72832201600002, -37.51847599450001], [143.72953299199997, -37.518827994], [143.731409248, -37.5196250295], [143.74158191999996, -37.52394096850001], [143.74465801599996, -37.507245995], [143.744836992, -37.5073300035], [143.74698201599998, -37.50843099400001], [143.759412, -37.51404400499999], [143.75962499199997, -37.514161998], [143.7617, -37.51511099250001], [143.76919951999997, -37.518495771], [143.78144732800004, -37.524028307], [143.802864992, -37.533702993000006], [143.802104992, -37.5377770075], [143.80090915200003, -37.539902861], [143.802615616, -37.5401091175], [143.81233801600004, -37.541365026999905], [143.811262016, -37.54215799250001], [143.811060992, -37.54234299250001], [143.810756992, -37.542681005999995], [143.810520992, -37.543014006], [143.810268992, -37.5434859965], [143.810144992, -37.5438130025], [143.810040992, -37.544208995], [143.809943008, -37.544697006499995], [143.809838656, -37.54527233800001], [143.80959308799999, -37.546617843], [143.81330985600002, -37.54706620899999], [143.813226304, -37.547505991], [143.81456079999998, -37.54766758849999], [143.81390499199998, -37.5511020025], [143.81353622399996, -37.55309907749999], [143.813491584, -37.5534117645], [143.81486959999998, -37.55402176499999], [143.81643856000002, -37.554715737], [143.818957568, -37.555831731], [143.8188504, -37.555959362500005], [143.818637728, -37.5563538195], [143.81856787200002, -37.55649943299999], [143.81853328, -37.556804942], [143.818488768, -37.55714697], [143.818271648, -37.5581605665], [143.818181344, -37.559005590999995], [143.817871488, -37.560661896], [143.817818144, -37.56100644]]]], 'type': 'MultiPolygon'}, 'id': 'AusByEPI2011_DataProfile.539', 'properties': {'feature_name': 'Alfredton', 'obese_p_me_2_rate_3_11_7_13': 27.2960275951921, 'feature_code': '201011001', 'smokers_me_2_rate_3_11_7_13': 16.7652059013177, 'SA2_Name_2011': 'Alfredton', 'SA2_Code_2011': 201011001, 'ovrwght_p_me_2_rate_3_11_7_13': 36.4348585131667, 'Average_taxable_income': 43846.169689999995}, 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('sa2.json') as sa2:\n",
    "    data = json.load(sa2)\n",
    "    code_dict = {}\n",
    "    for feature in data['features']:\n",
    "        print (feature)\n",
    "        code_dict[feature['properties']['SA2_Code_2011']] = feature\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "positive_tweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negative_tweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_word = list(stopwords.words())\n",
    "\n",
    "# remove nonalphabetic by regular expression\n",
    "def remove_nonalphabet(tweet_corpus):\n",
    "    for tweet in tweet_corpus:\n",
    "        for i in range(len(tweet)):\n",
    "            token = re.sub('[^a-zA-Z]',\"\",tweet[i])\n",
    "            tweet[i] = token\n",
    "\n",
    "        while '' in tweet:     # remove tokens contain only non-alphabet words\n",
    "            tweet.remove('')\n",
    "    return tweet_corpus\n",
    "\n",
    "# remove stopword \n",
    "def remove_stopword(tweet_corpus):\n",
    "    new_corpus = []\n",
    "    for tweet in tweet_corpus:\n",
    "        token = []\n",
    "        for word in tweet:\n",
    "            if word.lower() not in stop_word:\n",
    "                token.append(word)\n",
    "        new_corpus.append(token)\n",
    "    return new_corpus\n",
    "\n",
    "\n",
    "positive_tweets = remove_nonalphabet(positive_tweets)      \n",
    "negative_tweets = remove_nonalphabet(negative_tweets)\n",
    "\n",
    "\n",
    "positive_tweets = remove_stopword(positive_tweets)\n",
    "negative_tweets = remove_stopword(negative_tweets)\n",
    "\n",
    "# randomly split train/test set for positive tweets and negative tweets\n",
    "positive_train,positive_test,negative_train,negative_test = train_test_split(positive_tweets,negative_tweets,test_size = 0.1, train_size = 0.8)\n",
    "\n",
    "# develop data = dataset - traindata - testdata\n",
    "positive_set = positive_train + positive_test\n",
    "positive_develop = [x for x in positive_tweets if x not in positive_set]\n",
    "\n",
    "negative_set = negative_train + negative_test\n",
    "negative_develop = [x for x in negative_tweets if x not in negative_set]\n",
    "\n",
    "tweets_train = positive_train + negative_train\n",
    "tweets_test = positive_test + negative_test\n",
    "tweets_develop = positive_develop + negative_develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Naive Bayes tuning parameter alpha:\n",
      "best alpha is 1.000000\n",
      "alpha =    0.0001 has accuracy 0.722966\n",
      "alpha =    0.0010 has accuracy 0.723996\n",
      "alpha =    0.0100 has accuracy 0.726056\n",
      "alpha =    0.1000 has accuracy 0.733265\n",
      "alpha =    1.0000 has accuracy 0.742533\n",
      "alpha =   10.0000 has accuracy 0.737384\n",
      "\n",
      "\n",
      "For Logistic Regresstion tuning parameter C and Penalty:\n",
      "best C is 0.010000, best Penalty is l2\n",
      "C =    0.0001 and penalty = l1 show accuracy 0.501545\n",
      "C =    0.0001 and penalty = l2 show accuracy 0.714727\n",
      "C =    0.0010 and penalty = l1 show accuracy 0.501545\n",
      "C =    0.0010 and penalty = l2 show accuracy 0.703399\n",
      "C =    0.0100 and penalty = l1 show accuracy 0.501545\n",
      "C =    0.0100 and penalty = l2 show accuracy 0.707518\n",
      "C =    0.1000 and penalty = l1 show accuracy 0.650875\n",
      "C =    0.1000 and penalty = l2 show accuracy 0.721936\n",
      "C =    1.0000 and penalty = l1 show accuracy 0.707518\n",
      "C =    1.0000 and penalty = l2 show accuracy 0.730175\n",
      "C =   10.0000 and penalty = l1 show accuracy 0.706488\n",
      "C =   10.0000 and penalty = l2 show accuracy 0.725026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter\n",
    "\n",
    "# build frequency dictionary\n",
    "train_dict = []\n",
    "c = Counter()\n",
    "for tweet in tweets_train:\n",
    "    train_dict.append({i:tweet.count(i) for i in set(tweet)})\n",
    "\n",
    "test_dict = []\n",
    "c = Counter()\n",
    "for tweet in tweets_test:\n",
    "    test_dict.append({i:tweet.count(i) for i in set(tweet)})\n",
    "\n",
    "develop_dict = []\n",
    "c = Counter()\n",
    "for tweet in tweets_develop:\n",
    "    develop_dict.append({i:tweet.count(i) for i in set(tweet)})\n",
    "\n",
    "# prepare data for classifier\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_ = vectorizer.fit_transform(train_dict)\n",
    "test_ = vectorizer.transform(test_dict)\n",
    "develop_ = vectorizer.transform(develop_dict)\n",
    "\n",
    "\n",
    "# get label for dataset\n",
    "def get_target(positive_list,negative_list):\n",
    "    result = []\n",
    "    for i in range(len(positive_list)):\n",
    "        result.append(\"positive\")\n",
    "    for i in range(len(negative_list)):\n",
    "        result.append(\"negative\")\n",
    "    return result\n",
    "      \n",
    "train_target = get_target(positive_train,negative_train)\n",
    "test_target = get_target(positive_test,negative_test)\n",
    "develop_target = get_target(positive_develop,negative_develop)\n",
    "\n",
    "\n",
    "# tuning parameters for NB\n",
    "alphas = np.array([0.0001,0.001,0.01,0.1,1,10])      # alternative parameter for Naive Bayes alpha\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "grid = GridSearchCV(estimator = clf_NB, param_grid = dict(alpha = alphas),scoring = 'accuracy') # tuning for best alpha\n",
    "grid.fit(develop_,develop_target)\n",
    "\n",
    "print(\"For Naive Bayes tuning parameter alpha:\")\n",
    "best_alpha = grid.best_estimator_.alpha\n",
    "print(\"best alpha is %f\"%(best_alpha))\n",
    "\n",
    "\n",
    "# prove optimal alpha is found\n",
    "for i in alphas:\n",
    "    clf_NB = MultinomialNB(alpha = i)\n",
    "    clf_NB.fit(train_,train_target)\n",
    "    result = clf_NB.score(develop_, develop_target)\n",
    "    print(\"alpha = %9.4f has accuracy %f\"%(i,result))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#tuning parameters for LR:\n",
    "parameter_c = np.array([0.0001,0.001,0.01,0.1,1,10])   # alternative parameter for Logistic Regression\n",
    "Penalty = ['l1','l2']\n",
    "clf_LR = LogisticRegression()\n",
    "grid2 = GridSearchCV(estimator = clf_LR, param_grid = dict(C = parameter_c,penalty = Penalty),scoring = 'accuracy')\n",
    "grid2.fit(develop_,develop_target)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"For Logistic Regresstion tuning parameter C and Penalty:\")\n",
    "best_c = grid2.best_estimator_.C\n",
    "best_pen = grid2.best_estimator_.penalty\n",
    "print(\"best C is %f, best Penalty is %s\"%(best_c,best_pen))\n",
    "\n",
    "# prove optimal parameters are found\n",
    "for i in parameter_c:\n",
    "    for pen in Penalty:\n",
    "        clf_LR = LogisticRegression(C= i,penalty = pen)\n",
    "        clf_LR.fit(train_,train_target)\n",
    "        result = clf_LR.score(develop_, develop_target)\n",
    "        print(\"C = %9.4f and penalty = %s show accuracy %f\"%(i,pen,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "\n",
      "accuracy       :  0.755000\n",
      "macro f-score  :  0.754410\n",
      "\n",
      "Logistic Regression:\n",
      "\n",
      "accuracy       :  0.720000\n",
      "macro f-score  :  0.715627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "clf_NB = MultinomialNB(alpha = best_alpha)\n",
    "clf_NB.fit(train_,train_target)\n",
    "NB_result = clf_NB.predict(test_)\n",
    "print(\"Naive Bayes:\\n\")\n",
    "print(\"accuracy       :  %f\"%(accuracy_score(test_target,NB_result)))\n",
    "print(\"macro f-score  :  %f\\n\"%(f1_score(test_target,NB_result,average = 'macro')))\n",
    "\n",
    "clf_LR = LogisticRegression(C = best_c,penalty = best_pen)\n",
    "clf_LR.fit(train_,train_target)\n",
    "LR_result = clf_LR.predict(test_)\n",
    "\n",
    "print(\"Logistic Regression:\\n\")\n",
    "print(\"accuracy       :  %f\"%(accuracy_score(test_target,LR_result)))\n",
    "print(\"macro f-score  :  %f\"%(f1_score(test_target,LR_result,average = 'macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
