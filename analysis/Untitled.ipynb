{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /Users/mengyang/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[Synset('anger.n.01'), Synset('anger.n.02'), Synset('wrath.n.02'), Synset('anger.v.01'), Synset('anger.v.02')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "nltk.download(\"twitter_samples\")\n",
    "syns = wn.synsets(\"anger\")\n",
    "print(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['son', 'of', 'bitch']\n",
      "[Sentence(\"son of bitch!!\"), Sentence(\"!\")]\n",
      "[('son', 'NN'), ('of', 'IN'), ('bitch', 'NN')]\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "example = TextBlob(\"son of bitch!!!\")\n",
    "print(example.words)\n",
    "print(example.sentences)\n",
    "print(example.tags)\n",
    "print(example.sentiment)\n",
    "print(example.detect_language())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "JSONDecodeError",
     "evalue": "Extra data: line 1 column 3623 (char 3622)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c4ff372b9849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# read metadata from first row\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# {\"total_rows\":3877777,\"offset\":805584,\"rows\":[\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mfirstRow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'0}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mnumRows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirstRow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'total_rows'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extra data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Extra data: line 1 column 3623 (char 3622)"
     ]
    }
   ],
   "source": [
    "lineNumber = 1\n",
    "with open('tinyTwitter.json', \"r\") as f:\n",
    "    # read metadata from first row\n",
    "    # {\"total_rows\":3877777,\"offset\":805584,\"rows\":[\n",
    "    firstRow = json.loads(f.readline().rstrip()[:-1] + '0}')\n",
    "    numRows = firstRow['total_rows']\n",
    "    \n",
    "    twitLine = f.readline()\n",
    "    while twitLine:  # while not end of file\n",
    "        #lineNumber += 1\n",
    "        # truncate the end of line (\\n)\n",
    "        twitLine = twitLine.rstrip()\n",
    "        if twitLine[-1] == ',':\n",
    "            # truncate the last character\n",
    "            # print('\",\" detected')\n",
    "            twitLine = twitLine[:-1]\n",
    "        if twitLine[0] == ']':\n",
    "            # ignore the last line ']}'\n",
    "            break\n",
    "\n",
    "        # print (\"process {} is processing ...\".format(rank))\n",
    "        twit = json.loads(twitLine)\n",
    "        twit_text = re.sub(r\"http\\S+\",\"\",twit['value']['properties']['text'])\n",
    "        print (twit_text)\n",
    "        blob = TextBlob(twit_text)\n",
    "        print(blob.words)\n",
    "        print (blob.sentiment)\n",
    "        print (type(blob.sentiment.polarity))\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'geometry': {'coordinates': [[[[143.817818144, -37.56100644], [143.8174912, -37.562841825], [143.817114528, -37.564763975], [143.81697561599998, -37.56545293350001], [143.81694867200002, -37.56565093900001], [143.81670928, -37.566769874500004], [143.81621244800002, -37.56861833900001], [143.81617311999997, -37.568888217], [143.815870752, -37.570438887], [143.810498528, -37.5698114965], [143.81040195200003, -37.5698978175], [143.801001568, -37.5687896675], [143.79427222399994, -37.56800291800001], [143.78298697600002, -37.5666879565], [143.78281488000002, -37.5666710475], [143.75557145599998, -37.56348046349999], [143.748011008, -37.562609002500004], [143.738198016, -37.5614719925], [143.71563801599999, -37.558854002], [143.707238272, -37.5578780345], [143.69367420799995, -37.5582220235], [143.692926464, -37.558273953], [143.69230656000002, -37.558404710999994], [143.68592915200003, -37.56013037250001], [143.68494601600003, -37.551535920000006], [143.68694112, -37.5407070375], [143.68699801600002, -37.54046600099999], [143.68885488, -37.530731079], [143.68925916799998, -37.5307830085], [143.690048288, -37.526354016000006], [143.690823008, -37.521999356500004], [143.69083488, -37.521911074500004], [143.69194511999999, -37.52203698550001], [143.69195951999998, -37.52194742699999], [143.692952032, -37.5163499745], [143.69296681600002, -37.5162609525], [143.70365100799995, -37.5175800025], [143.70397299199996, -37.517608992], [143.70445001599998, -37.518707004], [143.70466601599998, -37.519108010000004], [143.70476799999997, -37.5193530055], [143.70482099199998, -37.518987001499994], [143.70557507199996, -37.515013664], [143.706168, -37.511888995499994], [143.70626, -37.5114419985], [143.70935926399994, -37.51284872], [143.70985798399997, -37.5130749935], [143.71504499199997, -37.5146030009999], [143.72832201600002, -37.51847599450001], [143.72953299199997, -37.518827994], [143.731409248, -37.5196250295], [143.74158191999996, -37.52394096850001], [143.74465801599996, -37.507245995], [143.744836992, -37.5073300035], [143.74698201599998, -37.50843099400001], [143.759412, -37.51404400499999], [143.75962499199997, -37.514161998], [143.7617, -37.51511099250001], [143.76919951999997, -37.518495771], [143.78144732800004, -37.524028307], [143.802864992, -37.533702993000006], [143.802104992, -37.5377770075], [143.80090915200003, -37.539902861], [143.802615616, -37.5401091175], [143.81233801600004, -37.541365026999905], [143.811262016, -37.54215799250001], [143.811060992, -37.54234299250001], [143.810756992, -37.542681005999995], [143.810520992, -37.543014006], [143.810268992, -37.5434859965], [143.810144992, -37.5438130025], [143.810040992, -37.544208995], [143.809943008, -37.544697006499995], [143.809838656, -37.54527233800001], [143.80959308799999, -37.546617843], [143.81330985600002, -37.54706620899999], [143.813226304, -37.547505991], [143.81456079999998, -37.54766758849999], [143.81390499199998, -37.5511020025], [143.81353622399996, -37.55309907749999], [143.813491584, -37.5534117645], [143.81486959999998, -37.55402176499999], [143.81643856000002, -37.554715737], [143.818957568, -37.555831731], [143.8188504, -37.555959362500005], [143.818637728, -37.5563538195], [143.81856787200002, -37.55649943299999], [143.81853328, -37.556804942], [143.818488768, -37.55714697], [143.818271648, -37.5581605665], [143.818181344, -37.559005590999995], [143.817871488, -37.560661896], [143.817818144, -37.56100644]]]], 'type': 'MultiPolygon'}, 'id': 'AusByEPI2011_DataProfile.539', 'properties': {'feature_name': 'Alfredton', 'obese_p_me_2_rate_3_11_7_13': 27.2960275951921, 'feature_code': '201011001', 'smokers_me_2_rate_3_11_7_13': 16.7652059013177, 'SA2_Name_2011': 'Alfredton', 'SA2_Code_2011': 201011001, 'ovrwght_p_me_2_rate_3_11_7_13': 36.4348585131667, 'Average_taxable_income': 43846.169689999995}, 'type': 'Feature'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('sa2.json') as sa2:\n",
    "    data = json.load(sa2)\n",
    "    code_dict = {}\n",
    "    for feature in data['features']:\n",
    "        print (feature)\n",
    "        code_dict[feature['properties']['SA2_Code_2011']] = feature\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "positive_tweets = nltk.corpus.twitter_samples.tokenized(\"positive_tweets.json\")\n",
    "negative_tweets = nltk.corpus.twitter_samples.tokenized(\"negative_tweets.json\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_word = list(stopwords.words())\n",
    "\n",
    "# remove nonalphabetic by regular expression\n",
    "def remove_nonalphabet(tweet_corpus):\n",
    "    for tweet in tweet_corpus:\n",
    "        print(tweet)\n",
    "        for i in range(len(tweet)):\n",
    "            token = re.sub('[^a-zA-Z]',\"\",tweet[i])\n",
    "            tweet[i] = token\n",
    "\n",
    "        while '' in tweet:     # remove tokens contain only non-alphabet words\n",
    "            tweet.remove('')\n",
    "        print (tweet_corpus)\n",
    "    return tweet_corpus\n",
    "\n",
    "# remove stopword \n",
    "def remove_stopword(tweet_corpus):\n",
    "    new_corpus = []\n",
    "    for tweet in tweet_corpus:\n",
    "        token = []\n",
    "        for word in tweet:\n",
    "            if word.lower() not in stop_word:\n",
    "                token.append(word)\n",
    "        new_corpus.append(token)\n",
    "    return new_corpus\n",
    "\n",
    "\n",
    "positive_tweets = remove_nonalphabet(positive_tweets)      \n",
    "negative_tweets = remove_nonalphabet(negative_tweets)\n",
    "\n",
    "\n",
    "positive_tweets = remove_stopword(positive_tweets)\n",
    "negative_tweets = remove_stopword(negative_tweets)\n",
    "\n",
    "# randomly split train/test set for positive tweets and negative tweets\n",
    "positive_train,positive_test,negative_train,negative_test = train_test_split(positive_tweets,negative_tweets,test_size = 0.1, train_size = 0.8)\n",
    "\n",
    "# develop data = dataset - traindata - testdata\n",
    "positive_set = positive_train + positive_test\n",
    "positive_develop = [x for x in positive_tweets if x not in positive_set]\n",
    "\n",
    "negative_set = negative_train + negative_test\n",
    "negative_develop = [x for x in negative_tweets if x not in negative_set]\n",
    "\n",
    "tweets_train = positive_train + negative_train\n",
    "tweets_test = positive_test + negative_test\n",
    "tweets_develop = positive_develop + negative_develop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Sample sequence X is empty.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-74222be05246>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtrain_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtest_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mdevelop_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevelop_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    289\u001b[0m         \"\"\"\n\u001b[1;32m    290\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfitting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/dict_vectorizer.py\u001b[0m in \u001b[0;36m_transform\u001b[0;34m(self, X, fitting)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindptr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Sample sequence X is empty.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Sample sequence X is empty."
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from collections import Counter\n",
    "\n",
    "# build frequency dictionary\n",
    "train_dict = []\n",
    "c = Counter()\n",
    "for tweet in tweets_train:\n",
    "    train_dict.append({i:tweet.count(i) for i in set(tweet)})\n",
    "\n",
    "test_dict = []\n",
    "c = Counter()\n",
    "for tweet in tweets_test:\n",
    "    test_dict.append({i:tweet.count(i) for i in set(tweet)})\n",
    "develop_dict = []\n",
    "c = Counter()\n",
    "for tweet in tweets_develop:\n",
    "    develop_dict.append({i:tweet.count(i) for i in set(tweet)})\n",
    "\n",
    "# prepare data for classifier\n",
    "vectorizer = DictVectorizer()\n",
    "\n",
    "train_ = vectorizer.fit_transform(train_dict)\n",
    "test_ = vectorizer.transform(test_dict)\n",
    "develop_ = vectorizer.transform(develop_dict)\n",
    "\n",
    "\n",
    "# get label for dataset\n",
    "def get_target(positive_list,negative_list):\n",
    "    result = []\n",
    "    for i in range(len(positive_list)):\n",
    "        result.append(\"positive\")\n",
    "    for i in range(len(negative_list)):\n",
    "        result.append(\"negative\")\n",
    "    return result\n",
    "      \n",
    "train_target = get_target(positive_train,negative_train)\n",
    "test_target = get_target(positive_test,negative_test)\n",
    "develop_target = get_target(positive_develop,negative_develop)\n",
    "\n",
    "\n",
    "# tuning parameters for NB\n",
    "alphas = np.array([0.0001,0.001,0.01,0.1,1,10])      # alternative parameter for Naive Bayes alpha\n",
    "clf_NB = MultinomialNB()\n",
    "\n",
    "grid = GridSearchCV(estimator = clf_NB, param_grid = dict(alpha = alphas),scoring = 'accuracy') # tuning for best alpha\n",
    "grid.fit(develop_,develop_target)\n",
    "\n",
    "print(\"For Naive Bayes tuning parameter alpha:\")\n",
    "best_alpha = grid.best_estimator_.alpha\n",
    "print(\"best alpha is %f\"%(best_alpha))\n",
    "\n",
    "\n",
    "# prove optimal alpha is found\n",
    "for i in alphas:\n",
    "    clf_NB = MultinomialNB(alpha = i)\n",
    "    clf_NB.fit(train_,train_target)\n",
    "    result = clf_NB.score(develop_, develop_target)\n",
    "    print(\"alpha = %9.4f has accuracy %f\"%(i,result))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "#tuning parameters for LR:\n",
    "parameter_c = np.array([0.0001,0.001,0.01,0.1,1,10])   # alternative parameter for Logistic Regression\n",
    "Penalty = ['l1','l2']\n",
    "clf_LR = LogisticRegression()\n",
    "grid2 = GridSearchCV(estimator = clf_LR, param_grid = dict(C = parameter_c,penalty = Penalty),scoring = 'accuracy')\n",
    "grid2.fit(develop_,develop_target)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"For Logistic Regresstion tuning parameter C and Penalty:\")\n",
    "best_c = grid2.best_estimator_.C\n",
    "best_pen = grid2.best_estimator_.penalty\n",
    "print(\"best C is %f, best Penalty is %s\"%(best_c,best_pen))\n",
    "\n",
    "# prove optimal parameters are found\n",
    "for i in parameter_c:\n",
    "    for pen in Penalty:\n",
    "        clf_LR = LogisticRegression(C= i,penalty = pen)\n",
    "        clf_LR.fit(train_,train_target)\n",
    "        result = clf_LR.score(develop_, develop_target)\n",
    "        print(\"C = %9.4f and penalty = %s show accuracy %f\"%(i,pen,result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes:\n",
      "\n",
      "accuracy       :  0.755000\n",
      "macro f-score  :  0.754410\n",
      "\n",
      "Logistic Regression:\n",
      "\n",
      "accuracy       :  0.720000\n",
      "macro f-score  :  0.715627\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "\n",
    "clf_NB = MultinomialNB(alpha = best_alpha)\n",
    "clf_NB.fit(train_,train_target)\n",
    "NB_result = clf_NB.predict(test_)\n",
    "print(\"Naive Bayes:\\n\")\n",
    "print(\"accuracy       :  %f\"%(accuracy_score(test_target,NB_result)))\n",
    "print(\"macro f-score  :  %f\\n\"%(f1_score(test_target,NB_result,average = 'macro')))\n",
    "\n",
    "clf_LR = LogisticRegression(C = best_c,penalty = best_pen)\n",
    "clf_LR.fit(train_,train_target)\n",
    "LR_result = clf_LR.predict(test_)\n",
    "\n",
    "print(\"Logistic Regression:\\n\")\n",
    "print(\"accuracy       :  %f\"%(accuracy_score(test_target,LR_result)))\n",
    "print(\"macro f-score  :  %f\"%(f1_score(test_target,LR_result,average = 'macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pkl_filename = \"sentiment_model.pkl\"  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(clf_NB, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': 1, 'spin': 1, 'osteopath': 1, 'trainers': 1, 'well': 1, 'massage': 1, 'tomorrow': 1, 'sports': 1, 'running': 1, 'Running': 1, 'ouch': 1, 'help': 1, 'yoga': 1, 'rjcarchives': 1, 'new': 1}, {'soundtrack': 1, 'epic': 1, 'great': 1, 'one': 1, 'Kimimi': 1, 'listen': 1, 'enough': 1, 'reason': 1}, {'Whatsapp': 1, 'Paris': 1, 'one': 1, 'park': 1, 'LOL': 1, 'French': 1, 'head': 1, 'grab': 1, 'sure': 1, 'anything': 1, 'roommate': 1}, {'zzz': 1, 'BellisimoBella': 1, 'CassieSpaniel': 1, 'xx': 1, 'thanks': 1, 'SpanielHarry': 1, 'kevinthewhippet': 1, 'BrackenNelson': 1}, {'MUCH': 2, 'MISS': 1, 'BABY': 1, 'LOVE': 1, 'BIRTHDAY': 1, 'HAPPY': 1, 'fwmkian': 1}]\n"
     ]
    }
   ],
   "source": [
    "print (test_dict[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11239)\t1.0\n",
      "  (0, 14302)\t1.0\n",
      "  (0, 14888)\t1.0\n",
      "  (0, 15242)\t1.0\n",
      "  (0, 16400)\t1.0\n",
      "  (0, 17966)\t1.0\n",
      "  (0, 18635)\t1.0\n",
      "  (0, 18973)\t1.0\n",
      "  (1, 10073)\t1.0\n",
      "  (1, 10095)\t1.0\n",
      "  (1, 10931)\t1.0\n",
      "  (1, 13925)\t1.0\n",
      "  (1, 15154)\t1.0\n",
      "  (1, 16063)\t1.0\n",
      "  (2, 2276)\t1.0\n",
      "  (2, 3683)\t1.0\n",
      "  (2, 4987)\t1.0\n",
      "  (2, 7016)\t1.0\n",
      "  (2, 7708)\t1.0\n",
      "  (2, 10899)\t1.0\n",
      "  (2, 11180)\t1.0\n",
      "  (2, 15154)\t1.0\n",
      "  (2, 15350)\t1.0\n",
      "  (2, 17472)\t1.0\n",
      "  (3, 17752)\t1.0\n",
      "  (3, 18895)\t1.0\n",
      "  (4, 478)\t1.0\n",
      "  (4, 541)\t1.0\n",
      "  (4, 2637)\t1.0\n",
      "  (4, 3701)\t1.0\n",
      "  (4, 4000)\t1.0\n",
      "  (4, 4037)\t2.0\n",
      "  (4, 10652)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print (test_[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(144.5940227, -37.3299798)\n"
     ]
    }
   ],
   "source": [
    "coordinate = \"144.5940227,-37.3299798\"\n",
    "print (tuple([float(i) for i in coordinate.split(',')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[[150.520929,-34.118347', ',[150.520929,-33.578141', ',[151.343021,-33.578141', ',[151.343021,-34.118347', '', '']\n"
     ]
    }
   ],
   "source": [
    "polygon=\"[[150.520929,-34.118347],[150.520929,-33.578141],[151.343021,-33.578141],[151.343021,-34.118347]]\"\n",
    "\n",
    "print (polygon.replace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set(['a','b','c'])\n",
    "b = set(['b','e'])\n",
    "len(a.intersection(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "a = \"a, b\"\n",
    "A,B = a.replace(\" \",\"\").split(',')\n",
    "print (B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a f g']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = \"a f g\"\n",
    "a.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
